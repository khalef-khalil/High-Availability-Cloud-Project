# Cloud Project - High Availability Web Application

A production-ready, highly available web application with automated failover, distributed storage, and multi-host redundancy.

## üèóÔ∏è Architecture Overview

This project implements a **multi-host, multi-tier HA architecture** designed to survive complete physical host failures while maintaining service availability.

### Key Features

- ‚úÖ **Multi-Host Geographic Redundancy** - Services distributed across 2 physical machines
- ‚úÖ **Automatic Load Balancer Failover** - Keepalived with floating VIP
- ‚úÖ **Database Auto-Failover** - MySQL Router with automatic master/slave switching
- ‚úÖ **Distributed Object Storage** - MinIO site replication (active-active)
- ‚úÖ **Zero SPOF** - No single point of failure in the architecture
- ‚úÖ **Automated Backups** - Daily database backups with 7-day retention
- ‚úÖ **Monitoring Ready** - Prometheus node exporters on all VMs

---

## üìä Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              CLIENT MACHINE                                  ‚îÇ
‚îÇ                          (Any device on network)                             ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ                    Accesses: http://192.168.64.10                           ‚îÇ
‚îÇ                         (Virtual IP - Floating)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Keepalived VRRP (Active)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                                      ‚îÇ
        ‚ñº                                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      PHYSICAL HOST 1      ‚îÇ                    ‚îÇ      PHYSICAL HOST 2      ‚îÇ
‚îÇ      (PC/Mac 1)           ‚îÇ                    ‚îÇ      (PC/Mac 2)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                           ‚îÇ                    ‚îÇ                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   LB1 (MASTER)      ‚îÇ  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄVRRP Sync‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  ‚îÇ  ‚îÇ   LB2 (BACKUP)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   192.168.64.12     ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ   192.168.64.15     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - HAProxy         ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ   - HAProxy         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - Keepalived      ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ   - Keepalived      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   - MySQL Router    ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ   - MySQL Router    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   VIP: .10 (Active) ‚îÇ  ‚îÇ                    ‚îÇ  ‚îÇ   VIP: .10 (Standby)‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ            ‚îÇ              ‚îÇ                    ‚îÇ            ‚îÇ              ‚îÇ
‚îÇ            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ                    ‚îÇ            ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ            ‚îÇ             ‚îÇ‚îÇ                    ‚îÇ            ‚îÇ             ‚îÇ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ‚îÇ
‚îÇ  ‚îÇ   APP1          ‚îÇ     ‚îÇ‚îÇ                    ‚îÇ  ‚îÇ   APP2          ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îÇ   192.168.64.13 ‚îÇ     ‚îÇ‚îÇ                    ‚îÇ  ‚îÇ   192.168.64.16 ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - Node.js     ‚îÇ     ‚îÇ‚îÇ                    ‚îÇ  ‚îÇ   - Node.js     ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îÇ   - Express     ‚îÇ     ‚îÇ‚îÇ                    ‚îÇ  ‚îÇ   - Express     ‚îÇ     ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ‚îÇ
‚îÇ           ‚îÇ              ‚îÇ‚îÇ                    ‚îÇ           ‚îÇ              ‚îÇ‚îÇ
‚îÇ           ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ           ‚îÇ              ‚îÇ‚îÇ
‚îÇ           ‚îÇ   ‚îÇ   MYSQL ROUTER       ‚îÇ         ‚îÇ           ‚îÇ              ‚îÇ‚îÇ
‚îÇ           ‚îÇ   ‚îÇ   Port 3306: R/W     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ‚îÇ
‚îÇ           ‚îÇ   ‚îÇ   Port 3307: R/O     ‚îÇ         ‚îÇ                          ‚îÇ‚îÇ
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ                          ‚îÇ‚îÇ
‚îÇ           ‚îÇ              ‚îÇ                     ‚îÇ                          ‚îÇ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ                     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ‚îÇ
‚îÇ  ‚îÇ   MDB (MASTER)  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ  ‚îÇ   SDB (SLAVE)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ  ‚îÇ   192.168.64.14 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄReplication‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚ñ∫‚îÇ   192.168.64.17 ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   - MySQL 8.0   ‚îÇ                           ‚îÇ  ‚îÇ   - MySQL 8.0   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   - Master DB   ‚îÇ                           ‚îÇ  ‚îÇ   - Read Replica‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   - MinIO Site1 ‚îÇ‚óÑ‚îÄ‚îÄSite Replication‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚ñ∫‚îÇ   - MinIO Site2 ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   Port 9000     ‚îÇ    (Active-Active)        ‚îÇ  ‚îÇ   Port 9000     ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                 ‚îÇ                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

        ‚ñ≤                                              ‚ñ≤
        ‚îÇ                                              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ If Host 1 fails, Host 2 continues ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ If Host 2 fails, Host 1 continues ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Component Distribution

### Physical Host 1 (Primary Site)
| VM Name | IP Address      | Services                          | RAM  | Role        |
|---------|-----------------|-----------------------------------|------|-------------|
| LB1     | 192.168.64.12   | HAProxy, Keepalived, MySQL Router | 1.4G | Primary LB  |
| APP1    | 192.168.64.13   | Node.js, Express                  | 1.4G | App Server  |
| MDB     | 192.168.64.14   | MySQL Master, MinIO Site1         | 1.4G | Master DB   |

### Physical Host 2 (Secondary Site)
| VM Name | IP Address      | Services                          | RAM  | Role        |
|---------|-----------------|-----------------------------------|------|-------------|
| LB2     | 192.168.64.15   | HAProxy, Keepalived, MySQL Router | 1.4G | Backup LB   |
| APP2    | 192.168.64.16   | Node.js, Express                  | 1.4G | App Server  |
| SDB     | 192.168.64.17   | MySQL Slave, MinIO Site2          | 1.4G | Replica DB  |

### Network Configuration
| Component      | IP/Port           | Purpose                          |
|----------------|-------------------|----------------------------------|
| Virtual IP     | 192.168.64.10     | Floating IP (Keepalived)         |
| HAProxy        | :80               | HTTP Load Balancer               |
| HAProxy Stats  | :8404/stats       | Monitoring Dashboard             |
| MySQL Router   | :3306             | R/W Database Connections         |
| MySQL Router   | :3307             | R/O Database Connections         |
| MySQL Master   | 192.168.64.14:3306| Write Operations                 |
| MySQL Slave    | 192.168.64.17:3306| Read Operations                  |
| MinIO Site1    | 192.168.64.14:9000| Object Storage (Primary)         |
| MinIO Site2    | 192.168.64.17:9000| Object Storage (Replica)         |
| MinIO Console  | :9001             | Web UI                           |

---

## üîÑ Failover Scenarios

### Scenario 1: Host 1 Complete Failure

**What Happens:**
1. ‚ö†Ô∏è  Host 1 goes offline (power loss, network failure, etc.)
2. üîÑ Keepalived on LB2 detects LB1 failure (within 1-2 seconds)
3. ‚úÖ LB2 assumes the Virtual IP (192.168.64.10)
4. ‚úÖ HAProxy on LB2 routes traffic to APP2
5. ‚úÖ MySQL Router on LB2 connects APP2 to SDB (slave becomes write-capable)
6. ‚úÖ MinIO Site2 serves all file requests
7. ‚è±Ô∏è  **Total Downtime: 2-5 seconds**

**What Remains Available:**
- ‚úÖ Web application fully functional
- ‚úÖ Database reads and writes (via slave)
- ‚úÖ All file uploads/downloads (via MinIO Site2)
- ‚úÖ All monitoring (node exporters)

**Manual Recovery Steps:**
```bash
# On SDB: Stop replication to allow writes
mysql> STOP SLAVE;
mysql> RESET SLAVE ALL;

# When Host 1 returns, rebuild replication
# (See replication setup guide below)
```

### Scenario 2: Host 2 Complete Failure

**What Happens:**
1. ‚ö†Ô∏è  Host 2 goes offline
2. ‚úÖ LB1 already has the VIP (no change needed)
3. ‚úÖ HAProxy on LB1 detects APP2 failure, routes to APP1 only
4. ‚úÖ MySQL Router connects to MDB (master)
5. ‚úÖ MinIO Site1 serves all file requests
6. ‚è±Ô∏è  **Total Downtime: 0 seconds** (reduced capacity only)

**What Remains Available:**
- ‚úÖ Web application fully functional
- ‚úÖ Database reads and writes (via master)
- ‚úÖ All file uploads/downloads (via MinIO Site1)

**Impact:**
- ‚ö†Ô∏è  Reduced to single app server (50% capacity)
- ‚ö†Ô∏è  Database reads hit master (increased load)
- ‚ö†Ô∏è  No database redundancy until Host 2 returns

### Scenario 3: Individual Service Failures

| Failed Service        | Detection Time | Failover Action                       | Impact          |
|-----------------------|----------------|---------------------------------------|-----------------|
| APP1                  | 10 seconds     | HAProxy routes to APP2 only           | 50% capacity    |
| APP2                  | 10 seconds     | HAProxy routes to APP1 only           | 50% capacity    |
| MySQL Master (MDB)    | 2-3 seconds    | Router fails over to Slave            | Brief write lag |
| MySQL Slave (SDB)     | 2-3 seconds    | Router uses Master for reads          | Increased load  |
| MinIO Site1           | N/A            | Manual switch to Site2 in .env        | Manual failover |
| MinIO Site2           | N/A            | Manual switch to Site1 in .env        | Manual failover |
| HAProxy on LB1        | 1-2 seconds    | Keepalived moves VIP to LB2           | ~2s downtime    |
| HAProxy on LB2        | N/A            | LB1 already handles traffic           | No impact       |

---

## üöÄ Quick Start

### Prerequisites
- 2 Physical machines (Mac/PC) on the same network
- Virtualization software (UTM, VirtualBox, VMware, etc.)
- Ubuntu 24.04 LTS ARM64 or AMD64 images
- Minimum 6GB RAM per physical host
- Static IPs or DHCP reservations for all VMs

### Deployment Steps

1. **Set up VMs on both hosts**
   ```bash
   # On Host 1: Create LB1, APP1, MDB
   # On Host 2: Create LB2, APP2, SDB
   ```

2. **Run setup scripts** (SSH into each VM)
   ```bash
   # On LB1 and LB2
   ./scripts/setup-lb.sh

   # On APP1
   ./scripts/setup-app.sh 1

   # On APP2
   ./scripts/setup-app.sh 2

   # On MDB
   ./scripts/setup-db-master.sh

   # On SDB
   ./scripts/setup-db-slave.sh
   ./scripts/setup-replication.sh
   ```

3. **Deploy application**
   ```bash
   ./scripts/deploy-app.sh
   ```

4. **Access from client machine**
   ```bash
   # From any device on the network
   open http://192.168.64.10
   ```

---

## üîß Configuration Details

### Load Balancer (HAProxy + Keepalived)

**HAProxy Configuration** (`/etc/haproxy/haproxy.cfg`):
```
backend app_nodes
    balance roundrobin
    option httpchk GET /health
    server app1 192.168.64.13:8000 check inter 10s
    server app2 192.168.64.16:8000 check inter 10s
```

**Keepalived VIP**:
- Virtual IP: `192.168.64.10/24`
- Master: LB1 (priority 100)
- Backup: LB2 (priority 90)
- Health check: HAProxy service status every 2s

### Database Layer

**MySQL Router** (on both LB1 and LB2):
```
[routing:primary]
destinations = 192.168.64.14:3306,192.168.64.17:3306
routing_strategy = first-available
mode = read-write

[routing:secondary]  
destinations = 192.168.64.17:3306,192.168.64.14:3306
routing_strategy = round-robin
mode = read-only
```

**Replication**:
- Type: Master-Slave (Asynchronous)
- Master: MDB (192.168.64.14)
- Slave: SDB (192.168.64.17)
- Lag: ~0 seconds under normal load
- Auto-reconnect: Enabled

### Storage Layer (MinIO)

**Site Replication**:
- Site1: http://192.168.64.14:9000
- Site2: http://192.168.64.17:9000
- Mode: Active-Active (bidirectional sync)
- Bucket: `uploads`
- Replication Status: `REPLICA`

**Application Configuration**:
```bash
MINIO_ENDPOINT=192.168.64.14
MINIO_PORT=9000
MINIO_BUCKET=uploads
```

---

## üìà Monitoring

### Health Checks

```bash
# Check all services
./scripts/monitor.sh

# Check individual components
curl http://192.168.64.10/health          # App health
curl http://192.168.64.10:8404/stats      # HAProxy stats
curl http://192.168.64.10/api/info        # App info

# Check MySQL Router
mysql -h 192.168.64.12 -P 3306 -u cloud_user -p -e 'SELECT @@hostname'

# Check replication status
ssh cloud-sdb@192.168.64.17 "mysql -e 'SHOW SLAVE STATUS\G' | grep Running"

# Check MinIO replication
ssh cloud-mdb@192.168.64.14 "/usr/local/bin/mc admin replicate info site1"
```

### Prometheus Metrics

Node exporters running on all VMs at port `:9100`:
- `http://192.168.64.12:9100/metrics` - LB1
- `http://192.168.64.13:9100/metrics` - APP1
- `http://192.168.64.14:9100/metrics` - MDB
- `http://192.168.64.15:9100/metrics` - LB2
- `http://192.168.64.16:9100/metrics` - APP2
- `http://192.168.64.17:9100/metrics` - SDB

**Prometheus Configuration** (on your Mac):
```yaml
scrape_configs:
  - job_name: 'cloud-infrastructure'
    static_configs:
      - targets:
        - '192.168.64.12:9100'
        - '192.168.64.13:9100'
        - '192.168.64.14:9100'
        - '192.168.64.15:9100'
        - '192.168.64.16:9100'
        - '192.168.64.17:9100'
```

---

## üíæ Backup & Recovery

### Automated Backups

**Database Backups**:
- Master: Daily at 2:00 AM ‚Üí `/var/backups/mysql/cloud_app_YYYYMMDD_HHMMSS.sql.gz`
- Slave: Daily at 3:00 AM ‚Üí `/var/backups/mysql/cloud_app_slave_YYYYMMDD_HHMMSS.sql.gz`
- Retention: 7 days
- Format: Compressed SQL dump

**Backup Script Location**: `/usr/local/bin/mysql-backup.sh`

**Manual Backup**:
```bash
# On MDB or SDB
sudo /usr/local/bin/mysql-backup.sh

# Restore from backup
gunzip < backup_file.sql.gz | mysql cloud_app
```

### Disaster Recovery

**Full System Recovery**:
1. Restore VMs on new hardware
2. Restore database from latest backup
3. Rebuild replication
4. Verify MinIO data (automatically replicated)
5. Test all health endpoints

**Recovery Time Objective (RTO)**: ~30 minutes  
**Recovery Point Objective (RPO)**: Last backup (< 24 hours)

---

## üß™ Testing

### API Endpoints

```bash
# Health check
curl http://192.168.64.10/health

# Get server info
curl http://192.168.64.10/api/info

# Get latest record
curl http://192.168.64.10/api/latest

# Upload test
curl -X POST http://192.168.64.10/api/upload \
  -F "name=Test" \
  -F "image=@/path/to/image.jpg"
```

### Load Testing

```bash
# Install Apache Bench
sudo apt-get install apache2-utils

# Test with 1000 requests, 10 concurrent
ab -n 1000 -c 10 http://192.168.64.10/

# Test failover during load
# Terminal 1: Run load test
ab -n 10000 -c 50 http://192.168.64.10/

# Terminal 2: Simulate failure
ssh cloud-lb@192.168.64.12 "sudo systemctl stop haproxy"
# Watch VIP move to LB2 and requests continue
```

---

## üîí Security Considerations

### Current State
- ‚ö†Ô∏è  HTTP only (no SSL/TLS)
- ‚ö†Ô∏è  Database credentials in plaintext .env files
- ‚ö†Ô∏è  No rate limiting
- ‚ö†Ô∏è  No WAF (Web Application Firewall)

### Production Recommendations
1. **Add SSL/TLS** - Let's Encrypt or self-signed certificates
2. **Secrets Management** - HashiCorp Vault or AWS Secrets Manager
3. **Rate Limiting** - Configure in HAProxy
4. **Firewall Rules** - UFW on all VMs
5. **SSH Key Authentication** - Disable password auth
6. **VPN/Bastion** - Restrict direct VM access

---

## üìö Architecture Decisions

### Why This Design?

| Decision                   | Reasoning                                                    |
|----------------------------|--------------------------------------------------------------|
| 2 Physical Hosts           | Balance between HA and cost/complexity                       |
| Active-Active Apps         | Full resource utilization, better load distribution          |
| Master-Slave DB            | Simple, proven, adequate for read-heavy workloads            |
| MySQL Router               | Native MySQL solution, easier than ProxySQL on ARM64         |
| MinIO Site Replication     | Industry-standard S3 API, active-active capability           |
| Keepalived                 | Lightweight, proven VRRP implementation                      |
| Round-robin LB             | Simple, fair distribution (could use least-connections)      |
| 7-day backup retention     | Balance between storage cost and recovery options            |

### Trade-offs

**What We Gained**:
- ‚úÖ Survives complete host failure
- ‚úÖ Automatic failover for most services
- ‚úÖ No single point of failure
- ‚úÖ Distributed data storage
- ‚úÖ Production-ready architecture

**What We Sacrificed**:
- ‚ö†Ô∏è  Complexity (6 VMs vs simple single server)
- ‚ö†Ô∏è  Resource usage (6GB+ RAM total)
- ‚ö†Ô∏è  Manual MinIO failover (could be improved with MinIO Gateway)
- ‚ö†Ô∏è  Async replication (small data loss risk during master failure)

---

## üìñ Troubleshooting

### Common Issues

**VIP Not Accessible**:
```bash
# Check Keepalived status
systemctl status keepalived

# Check which node has VIP
ip addr show | grep 192.168.64.10

# Check VRRP logs
journalctl -u keepalived -f
```

**Database Replication Broken**:
```bash
# Check slave status
mysql -e "SHOW SLAVE STATUS\G" | grep Running

# If stopped, see error
mysql -e "SHOW SLAVE STATUS\G" | grep Last_Error

# Rebuild replication (see setup-replication.sh)
```

**MinIO Not Syncing**:
```bash
# Check replication status
mc admin replicate info site1

# Manual sync
mc mirror site1/uploads site2/uploads
```

**App Can't Connect to Database**:
```bash
# Check MySQL Router
systemctl status mysqlrouter
netstat -tlnp | grep 3306

# Test connection
mysql -h 192.168.64.12 -P 3306 -u cloud_user -p
```

---

## üéØ Performance Metrics

### Observed Performance

| Metric                    | Value              | Notes                        |
|---------------------------|--------------------|------------------------------|
| VIP Failover Time         | 1-2 seconds        | Keepalived VRRP              |
| DB Failover Time          | 2-3 seconds        | MySQL Router detection       |
| HAProxy Health Check      | 10 seconds         | Configurable                 |
| Replication Lag           | 0 seconds          | Under normal load            |
| Request Latency (p50)     | ~10ms              | Without database queries     |
| Request Latency (p99)     | ~50ms              | With database queries        |
| Max Throughput            | ~1000 req/s        | 2 app servers, round-robin   |
| MinIO Sync Time           | <1 second          | For small files (<1MB)       |

### Resource Usage (Idle)

| VM    | CPU   | RAM Usage | RAM Available | Disk Used |
|-------|-------|-----------|---------------|-----------|
| LB1   | 0%    | 280 MB    | 1.1 GB        | 4.8 GB    |
| LB2   | 0%    | 280 MB    | 1.1 GB        | 4.1 GB    |
| APP1  | 1%    | 315 MB    | 1.1 GB        | 4.9 GB    |
| APP2  | 1%    | 310 MB    | 1.1 GB        | 4.4 GB    |
| MDB   | 0%    | 630 MB    | 820 MB        | 4.6 GB    |
| SDB   | 0%    | 775 MB    | 680 MB        | 4.6 GB    |

---

## üó∫Ô∏è Roadmap

### Phase 1: Foundation ‚úÖ COMPLETE
- [x] Basic HA with load balancer
- [x] Database replication
- [x] Automated backups
- [x] MinIO object storage
- [x] Monitoring exporters

### Phase 2: Advanced HA ‚úÖ COMPLETE
- [x] Keepalived VIP failover
- [x] MySQL Router auto-failover
- [x] MinIO site replication
- [x] Multi-host distribution guide

### Phase 3: Enhancements (PLANNED)
- [ ] SSL/TLS encryption
- [ ] Redis caching layer
- [ ] Rate limiting
- [ ] Automated testing suite
- [ ] CI/CD pipeline

### Phase 4: Production Hardening (FUTURE)
- [ ] Secrets management
- [ ] WAF integration
- [ ] Log aggregation (ELK/Loki)
- [ ] Automated scaling
- [ ] Cross-region replication

---

## üõ†Ô∏è Multi-Host Deployment Guide

### Overview

This guide will help you split the current architecture across 2 physical machines for true geographic redundancy.

### Prerequisites

- **2 Physical Machines** on the same network:
  - Host 1: Mac/PC with 4GB+ RAM, 20GB+ free disk
  - Host 2: Mac/PC with 4GB+ RAM, 20GB+ free disk
- **Network**: Both hosts on same subnet (e.g., 192.168.64.x)
- **Virtualization**: UTM (Mac), VirtualBox, or VMware
- **Optional Client**: Third machine to access the application

### Architecture Split

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     NETWORK TOPOLOGY                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                            ‚îÇ
‚îÇ  HOST 1 (Primary Site)           HOST 2 (Secondary Site)  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ LB1  (.12)   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄVRRP‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ LB2  (.15)   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ APP1 (.13)   ‚îÇ                ‚îÇ APP2 (.16)   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ MDB  (.14)   ‚îÇ‚óÑ‚îÄ‚îÄReplication‚îÄ‚îÄ‚îÇ SDB  (.17)   ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                               ‚îÇ                 ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄVIP (.10)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                        ‚îÇ                                  ‚îÇ
‚îÇ                        ‚ñº                                  ‚îÇ
‚îÇ                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ                ‚îÇ   CLIENT     ‚îÇ                           ‚îÇ
‚îÇ                ‚îÇ  (Any IP)    ‚îÇ                           ‚îÇ
‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Step-by-Step Implementation

#### Step 1: Prepare Both Physical Hosts

**On Host 1:**
1. Install virtualization software (UTM/VirtualBox/VMware)
2. Download Ubuntu 24.04 LTS ARM64/AMD64 ISO
3. Create 3 VMs:
   - `cloud-lb` - 1.5GB RAM, 10GB disk
   - `cloud-app1` - 1.5GB RAM, 10GB disk
   - `cloud-mdb` - 1.5GB RAM, 10GB disk
4. Configure network: Bridged mode or shared network with Host 2
5. Set static IPs:
   - LB1: 192.168.64.12
   - APP1: 192.168.64.13
   - MDB: 192.168.64.14

**On Host 2:**
1. Repeat virtualization setup
2. Create 3 VMs:
   - `cloud-lb2` - 1.5GB RAM, 10GB disk
   - `cloud-app2` - 1.5GB RAM, 10GB disk
   - `cloud-sdb` - 1.5GB RAM, 10GB disk
3. Configure same network as Host 1
4. Set static IPs:
   - LB2: 192.168.64.15
   - APP2: 192.168.64.16
   - SDB: 192.168.64.17

#### Step 2: Network Configuration

**Option A: Bridged Network (Recommended)**
- Both hosts connect to same physical network
- VMs get IPs from network DHCP or static config
- Most reliable for production use

**Option B: Shared Virtual Network**
- Create virtual network spanning both hosts
- Requires network configuration on hypervisor
- More complex but isolated from main network

**Verify connectivity:**
```bash
# From Host 1, ping Host 2 VMs
ping 192.168.64.15  # LB2
ping 192.168.64.16  # APP2
ping 192.168.64.17  # SDB

# From Host 2, ping Host 1 VMs
ping 192.168.64.12  # LB1
ping 192.168.64.13  # APP1
ping 192.168.64.14  # MDB
```

#### Step 3: Install Base Services

**On all VMs (Host 1 and Host 2):**
```bash
# Update system
sudo apt-get update && sudo apt-get upgrade -y

# Install common tools
sudo apt-get install -y curl wget git htop net-tools vim

# Set up SSH keys between VMs for easier management
ssh-keygen -t ed25519
ssh-copy-id user@other-vm-ip
```

#### Step 4: Deploy Services to Host 1

**LB1 (192.168.64.12):**
```bash
# Clone repository
git clone <your-repo> /opt/cloud-app
cd /opt/cloud-app

# Run load balancer setup
sudo ./scripts/setup-lb.sh

# Configure as MASTER for Keepalived
# (Script should already configure this)

# Install MySQL Router
sudo apt-get install -y mysql-router
sudo cp /path/to/mysqlrouter.conf /etc/mysqlrouter/
sudo systemctl enable mysqlrouter
sudo systemctl start mysqlrouter
```

**APP1 (192.168.64.13):**
```bash
cd /opt/cloud-app
sudo ./scripts/setup-app.sh 1

# Deploy application code
sudo ./scripts/deploy-app.sh
```

**MDB (192.168.64.14):**
```bash
cd /opt/cloud-app
sudo ./scripts/setup-db-master.sh

# Install MinIO
wget https://dl.min.io/server/minio/release/linux-arm64/minio
sudo mv minio /usr/local/bin/
sudo chmod +x /usr/local/bin/minio

# Configure MinIO (as site1)
# See MinIO configuration in main docs
```

#### Step 5: Deploy Services to Host 2

**LB2 (192.168.64.15):**
```bash
cd /opt/cloud-app
sudo ./scripts/setup-lb.sh

# Configure as BACKUP for Keepalived
# Edit /etc/keepalived/keepalived.conf:
#   state BACKUP
#   priority 90

# Install MySQL Router
sudo apt-get install -y mysql-router
sudo cp /path/to/mysqlrouter.conf /etc/mysqlrouter/
sudo systemctl enable mysqlrouter
sudo systemctl start mysqlrouter
```

**APP2 (192.168.64.16):**
```bash
cd /opt/cloud-app
sudo ./scripts/setup-app.sh 2
sudo ./scripts/deploy-app.sh
```

**SDB (192.168.64.17):**
```bash
cd /opt/cloud-app
sudo ./scripts/setup-db-slave.sh

# Set up replication
sudo ./scripts/setup-replication.sh

# Install MinIO
wget https://dl.min.io/server/minio/release/linux-arm64/minio
sudo mv minio /usr/local/bin/
sudo chmod +x /usr/local/bin/minio

# Configure MinIO (as site2)
# Set up site replication with site1
```

#### Step 6: Configure Keepalived VIP

**On LB1:**
```bash
# Edit /etc/keepalived/keepalived.conf
sudo nano /etc/keepalived/keepalived.conf

# Configuration:
vrrp_instance VI_1 {
    state MASTER
    interface enp0s1  # Your network interface
    virtual_router_id 51
    priority 100      # Higher priority = master
    advert_int 1
    
    authentication {
        auth_type PASS
        auth_pass cloud2024
    }
    
    virtual_ipaddress {
        192.168.64.10/24
    }
}

# Restart Keepalived
sudo systemctl restart keepalived
```

**On LB2:**
```bash
# Same config but with:
#   state BACKUP
#   priority 90  # Lower than master
```

#### Step 7: Set Up Database Replication

**On MDB (Master):**
```bash
# Get master status
sudo mysql -e "SHOW MASTER STATUS"
# Note the File and Position values
```

**On SDB (Slave):**
```bash
sudo mysql << EOF
CHANGE MASTER TO
    MASTER_HOST='192.168.64.14',
    MASTER_USER='repl_user',
    MASTER_PASSWORD='repl_secure_2024',
    MASTER_LOG_FILE='mysql-bin.XXXXXX',  # From SHOW MASTER STATUS
    MASTER_LOG_POS=YYYY;                 # From SHOW MASTER STATUS

START SLAVE;
SHOW SLAVE STATUS\G
EOF
```

#### Step 8: Configure MinIO Site Replication

**On MDB:**
```bash
# Set up MinIO aliases
mc alias set site1 http://192.168.64.14:9000 minioadmin minio_secure_2024
mc alias set site2 http://192.168.64.17:9000 minioadmin minio_secure_2024

# Enable site replication
mc admin replicate add site1 site2

# Verify
mc admin replicate info site1
```

#### Step 9: Verify Multi-Host Setup

**Test 1: VIP Failover**
```bash
# Check which host has VIP
ssh cloud-lb@192.168.64.12 "ip addr show | grep 192.168.64.10"

# Stop LB1
ssh cloud-lb@192.168.64.12 "sudo systemctl stop keepalived"

# Verify VIP moved to LB2
ssh cloud-lb2@192.168.64.15 "ip addr show | grep 192.168.64.10"

# Access still works
curl http://192.168.64.10/health
```

**Test 2: Host 1 Complete Failure**
```bash
# Shut down all VMs on Host 1
# Or disconnect Host 1 from network

# From client machine, verify access continues
curl http://192.168.64.10/api/info
# Should return APP2 info

# Check database is writable
curl -X POST http://192.168.64.10/api/upload -F "name=Test" -F "image=@test.jpg"
```

**Test 3: Host 2 Failure**
```bash
# Shut down all VMs on Host 2

# From client, verify access continues
curl http://192.168.64.10/api/info
# Should return APP1 info
```

#### Step 10: Client Machine Setup

**Any device on the network:**

1. **Web Browser Access:**
   ```
   http://192.168.64.10
   ```

2. **API Testing:**
   ```bash
   # Health check
   curl http://192.168.64.10/health

   # Upload file
   curl -X POST http://192.168.64.10/api/upload \
     -F "name=MyFile" \
     -F "image=@/path/to/file.jpg"

   # Get latest
   curl http://192.168.64.10/api/latest
   ```

3. **Monitoring:**
   ```bash
   # HAProxy stats
   open http://192.168.64.10:8404/stats
   
   # Prometheus (if configured)
   open http://<your-mac-ip>:9090
   
   # Grafana (if configured)
   open http://<your-mac-ip>:3000
   ```

### Firewall Configuration

**On all VMs, configure UFW:**
```bash
# Allow SSH
sudo ufw allow 22/tcp

# On LB1 and LB2
sudo ufw allow 80/tcp      # HTTP
sudo ufw allow 8404/tcp    # HAProxy stats
sudo ufw allow 3306/tcp    # MySQL Router
sudo ufw allow 3307/tcp    # MySQL Router (read)

# On APP1 and APP2
sudo ufw allow 8000/tcp    # Node.js app

# On MDB and SDB
sudo ufw allow 3306/tcp    # MySQL
sudo ufw allow 9000/tcp    # MinIO
sudo ufw allow 9001/tcp    # MinIO console

# On all VMs (for Keepalived)
sudo ufw allow from 192.168.64.0/24 to any port 112 proto vrrp

# Enable firewall
sudo ufw enable
```

### Troubleshooting Multi-Host Issues

**VIP Not Failing Over:**
```bash
# Check both LB nodes can communicate
ping 192.168.64.12  # From LB2
ping 192.168.64.15  # From LB1

# Check VRRP multicasting
tcpdump -i enp0s1 vrrp

# Check Keepalived logs
journalctl -u keepalived -f
```

**Can't Access Services Between Hosts:**
```bash
# Check routing
traceroute 192.168.64.15  # From Host 1 to Host 2

# Check firewall
sudo ufw status verbose

# Test specific ports
telnet 192.168.64.14 3306  # MySQL
telnet 192.168.64.17 9000  # MinIO
```

**Replication Failing Across Hosts:**
```bash
# Check MySQL connectivity
mysql -h 192.168.64.14 -u repl_user -p  # From SDB

# Check MinIO site-to-site
mc admin info site1
mc admin info site2
```

### Performance Tuning for Multi-Host

**Network Latency Considerations:**
```bash
# Measure inter-host latency
ping -c 100 192.168.64.15  # From Host 1 to Host 2

# If latency > 10ms:
# - Increase health check intervals
# - Increase Keepalived timers
# - Use connection pooling more aggressively
```

**Replication Tuning:**
```bash
# On Master, increase binlog cache
SET GLOBAL binlog_cache_size = 1048576;

# On Slave, increase relay log size
SET GLOBAL relay_log_space_limit = 16777216;
```

### Backup Strategy for Multi-Host

**Automated Backups to External Storage:**
```bash
# On MDB, backup to NAS/external drive
cat > /etc/cron.d/mysql-remote-backup << 'EOF'
0 2 * * * root /usr/local/bin/mysql-backup.sh && \
  rsync -az /var/backups/mysql/ user@nas-ip:/backups/mysql-host1/
EOF

# On SDB, backup to same NAS
cat > /etc/cron.d/mysql-remote-backup << 'EOF'
0 3 * * * root /usr/local/bin/mysql-backup.sh && \
  rsync -az /var/backups/mysql/ user@nas-ip:/backups/mysql-host2/
EOF
```

---

## üéì Learning Resources

- [HAProxy Documentation](http://www.haproxy.org/)
- [Keepalived User Guide](https://www.keepalived.org/manpage.html)
- [MySQL Replication](https://dev.mysql.com/doc/refman/8.0/en/replication.html)
- [MySQL Router](https://dev.mysql.com/doc/mysql-router/8.0/en/)
- [MinIO Site Replication](https://min.io/docs/minio/linux/operations/install-deploy-manage/multi-site-replication.html)
- [Prometheus Monitoring](https://prometheus.io/docs/introduction/overview/)

---

## üìù License

MIT License - Feel free to use this architecture for your projects.

---

## üë• Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Test your changes thoroughly
4. Submit a pull request

---

## üìß Support

For issues or questions:
- Open an issue in the repository
- Check existing documentation
- Review troubleshooting section

---

**Built with ‚ù§Ô∏è for high availability and resilience**
